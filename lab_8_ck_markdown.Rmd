---
title: "Lab Week 8"
author: "Charlene Kormondy"
date: "March 8, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

###Load the packages
```{r}
library(tidyverse)
library(sf)
library(tmap)
library(leaflet)
library(spatstat)
library(maptools)


```


###Column graph of Texas Oil Spills
```{r}

oil_spills <- read_csv("oil_spills.csv")

df <- oil_spills %>% 
  filter(`Accident State` == "TX" & `Accident Year` < 2017) %>%  #have to bound it by angled apostrophies bc it has spaces in it
  group_by(`Accident Year`) %>% 
  summarise(Loss = sum (`Net Loss (Barrels)`)) #allows you to apply 1 or more functions to a dataset that is grouped

colnames(df) <- c("Year", "Loss")

ggplot(df, aes(x = Year, y = Loss)) +
  geom_col()

```

###Leaflet plot of spill locations in TX in 2016
```{r}

df_loc <- oil_spills %>% 
  filter(`Accident State` == "TX" & `Accident Year` == "2016") %>% 
  select(Latitude, Longitude, `Net Loss (Barrels)`)

colnames(df_loc) <- c("latitude", "longitude", "net_loss")

oil_sf <- st_as_sf(df_loc, coords = c("longitude", "latitude"), crs = 4326) #not all spatial things will automatically search for longitude latitude, some things need you to do it

leaflet(oil_sf) %>% 
  addTiles %>% 
  addMarkers()

```

##tmap plot with the Texas state shapefile
Sometimes we cant use this base map with leaflet, we need an actual shapefile
```{r}

states <- st_read(dsn = ".", layer = "states") #bc its alrady in our pathway
#now I can treat this information as a data frame. I can use my data frame functions like filter, select, etc. and it will keep spatial geometry stuff with it!

tex_boarder <- states %>% 
  filter(STATE_NAME == "Texas") %>% 
  st_transform(4326)

plot(tex_boarder)

tm_shape(tex_boarder) + #plot the texas boarder as a polygon
  tm_polygons() +
  tm_shape(oil_sf) +
  tm_dots(size = 0.3)

```

###Convert the data to spatial points patters (combination of point data and the bounding window bc we need to have a bounding window to evaluate them within)
we have to convert them into point data with a window that the current functions in r will work with (hopefuly, in the future, you wont have to do this)

```{r}

spill_sp <- as(oil_sf, "Spatial") #converted from simple features back to origional df. but r still doesnt recognize this as data that you want to do point analysis with
spill_ppp <- as(spill_sp, "ppp") #ppp is point pattern p

#state of texas outline is our bounding window:
tx_sp <- as(tex_boarder, "Spatial")
tx_owin <- as(tx_sp, "owin") #now it recognizes it as a window!

all_ppp <- ppp(spill_ppp$x, spill_ppp$y, window = tx_owin) #ppp fncn in the spatial stats package for creating point pattern

#4 points were rejected bc they were ouside of the boundary window

```

###A density plot:
```{r}
plot(density(all_ppp, sigma = 0.4)) #density plots are easy to make but can change a story. sigma = 1 and sigma = 0.1 is very different. You need a meaningful way to decide what you bandwidth (sigma) should be.
```

###Quadrat test for spatial evenness
Recall: it tests for spatial evenness (even though r says it tessts for spatial randomness)
```{r}
oil_qt <- quadrat.test(all_ppp, nx = 5, ny = 5) #5 regions that exist horizontally and 5 regions vertically
#this is a hypothesis test
oil_qt #we got a small p value. look at the data and we can tell that they are not csr observations (they dont look random because we see clustering and large blank areas)
#null: data point patters follow csr. we will reject the null and retain alt hypothesis that the data are not csr (not evenly distributed)

plot(all_ppp)
plot(oil_qt, add = TRUE, cex = 0.4) #plot the quadrat test
#if this region was spatially distributed, it whould have 14.5 in it. it also calculates the proportion of spatial area and for example, says there should be 11.6 total events in this quadrant. Upper right is expected ct, upper left is actual count, bottom number tells you have many sds higher or lower it is than the expected number

```

if data are clustered, the nearest neighbors will be closer on average than if they are evenly distributed. 

###G-Function for Nearest Neighbor Analysis
step 1: make a sewquence of values for r. Allison already found out what the best span is. In the future, we may need to figure this distance out for ourselves.
```{r}

r <- seq(0,1, by = 0.01)

#based on poison distrib, what would it look like if it were csr data?

oil_gfun <- envelope(all_ppp, fun = Gest, r= r, nsim = 100) #if you wanted to use L or K, it'd be L or Kest. we called it r above but we could have called it lag or something.)

#theoretical is theo col = model for csr, low and high cols

ggplot(oil_gfun, aes(x = r, y = obs)) +
  geom_line(color = "black") +
  geom_line(aes(x = r, y = theo), color = "red")

#this tells us that our observed data has a higher propotion of point pairs with neighbors at nearer distances compared to truley csr data

```

###Nearest neighbor using the L - function (Ripley's K, but standardized)
```{r}

r2 <- seq(0,3, by = 0.5)

oil_lfun <- envelope(all_ppp, fun = Lest, r = r2, nsim = 20, global = TRUE) #this is more computationally intensive bc instead of just finding a simgle nearest neighbor, it makes increasing bubbles around it until it includes all observations. TRUE so it shows all obs

ggplot(oil_lfun, aes(x = r2, y = obs)) +
  geom_line(color = "black") +
  geom_line(aes(x = r2, y = theo), color = "blue")

```
in 4 different ways, we determined that this is not csr data:
1. map
2. quadrat analysis told us we didnt have evenly distributed oil spills
3. L = nearest neighbor
4. K = global neighbors around them
